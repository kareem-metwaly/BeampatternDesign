auto_lr_find: false
<<<<<<< Updated upstream
lr: 1e-2
gradient_clip_val: 1
max_epochs: 50
=======
lr: 1e-3
max_epochs: 100
>>>>>>> Stashed changes
optimizer: "adam"
lr_scheduler:
  - "reduce on plateau"
  - min_lr: 1e-6

# related to fetching data
num_workers: 8
batch_size: 8
precision: 32
train_val_test_split: [0.70, 0.15, 0.15]
#random_seed: # Optional[int] = None: used for reproducing the same shuffling of the train, test and validation datasets.

# etc
gpus: [1]
checkpoints_dir: "./checkpoints/"
<<<<<<< Updated upstream
log_every_n_steps: 1
=======
log_every_n_steps: 5
>>>>>>> Stashed changes
